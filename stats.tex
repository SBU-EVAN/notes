\subsection{Tensions in Cosmology}
Define the posterior probability of parameters $\theta$ within model $\mathcal{M}$ given data $d$ as (Bayes' theorem)
\begin{equation}
    \mathcal{P}(\theta) := P(\theta|d,\mathcal{M}) = \frac{P(\theta|\mathcal{M})P(d|\theta,\mathcal{M})}{P(d|\mathcal{M})}
\end{equation}
The following notation will simplify this expression
\begin{equation*}
    \begin{split}
        \Pi(\theta) &= P(\theta|\mathcal{M}) \text{ , Prior probability} \\
        \mathcal{L}(\theta) &= P(d|\theta,\mathcal{M}) \text{ , the likelihood} \\
        \mathcal{E} &= P(d|\mathcal{M}) \text{ , the evidence}
    \end{split}
\end{equation*}
$$     \mathcal{P}(\theta) := P(\theta|d,\mathcal{M}) = \frac{\Pi(\theta)\mathcal{L}}{\mathcal{E}} $$
These are (maybe?) understandable as the prior probability is the parameters given the model, the likelihood as the data given some parameters of the model, and the evidence as the data give a model. From now on, the model $\mathcal{M}$ will be implied.

Consider now two data sets $d_1,d_2$. They have a joint likelihood $$\mathcal{L}(\theta) = P(d_1,d_2|\theta)$$
This \textbf{quantifies the likelihood of $d_1$ and $d_2$ coming from the same set of parameters of a given model.} Denote now $\mathcal{L}_1,\mathcal{P}_1$ as the marginalized likelihood and Posterior over $d_2$. (\textbf{Is there a procedure/algorithm to find the marginalized probability? Seems generally non-trivial}).

Now duplicate the parameter set to $\theta_1,\theta_2$ and perscibe a new joint likelihood $\mathcal{L}(\theta_1,\theta_2) = P(d_1,d_2|\theta_1,\theta_2)$.\textbf{ This is generally a choice, but how can one make a good choice?} The choice however is not unique.

As such, the constraints are imposed:
\begin{enumerate}
    \item $\mathcal{L}(\theta_1=\theta,\theta_2=\theta) = \mathcal{L}(\theta)$, or in plain english, the joint likelihood if $\theta_1=\theta=\theta_2$ needs to coincide with the likelihood of $\theta$
    \item $P(d_1|\theta_1,\theta_2)=P(d_1|\theta_1)$ once marginalized over $d_2$. Marginalizing the likelihood over one of the data sets removes the dependancy on the corresponding parameter sets.
\end{enumerate}
 \textbf{This ensures the datasets are conditionally independent,}
    $$ P(d_1,d_2|\theta) = P(d_1|\theta)P(d_2|\theta)$$
\textbf{Proof?}
Which means we can choose $$\mathcal{L}(\theta_1,\theta_2) = \mathcal{L}_1(\theta_1)\mathcal{L}_2(\theta_2)$$
If we further assume the prior distribution can be factorized, the joint posterior is then
$$ \mathcal{P}(\theta_1,\theta_2) = \mathcal{L}(\theta_1,\theta_2)\Pi(\theta_1)\Pi(\theta_2) $$
Defining $\Delta\theta = \theta_1-\theta_2$, the parameter difference posterior is given by
\[ \mathcal{P}(\Delta\theta) = \int_{V_\Pi} \mathcal{P}(\theta,\theta-\Delta\theta)d\theta\]